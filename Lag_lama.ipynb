{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gluonts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39h1RNKayXiJ",
        "outputId": "163ba706-dde1-4f27-a84f-82d701c13529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gluonts\n",
            "  Downloading gluonts-0.15.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: numpy~=1.16 in /usr/local/lib/python3.10/dist-packages (from gluonts) (1.26.4)\n",
            "Requirement already satisfied: pandas<3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gluonts) (2.1.4)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.10/dist-packages (from gluonts) (2.8.2)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.10/dist-packages (from gluonts) (4.66.5)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.10/dist-packages (from gluonts) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gluonts) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.0->gluonts) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.0->gluonts) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.0->gluonts) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.7->gluonts) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts) (1.16.0)\n",
            "Downloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gluonts\n",
            "Successfully installed gluonts-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/time-series-foundation-models/lag-llama/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPjdpdcT0Q4W",
        "outputId": "4d4cb7ca-5671-4c34-ba15-f97047d214e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lag-llama'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 328 (delta 113), reused 107 (delta 84), pack-reused 164\u001b[K\n",
            "Receiving objects: 100% (328/328), 234.57 KiB | 1.67 MiB/s, done.\n",
            "Resolving deltas: 100% (155/155), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/lag-llama\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmNAVi_j2z0n",
        "outputId": "30e2ad8e-ea0b-4a36-db4f-80faf9374a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lag-llama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt --quiet"
      ],
      "metadata": {
        "id": "P1hymXPI26F5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31cd606f-fca0-4d16-a483-4592cc733e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.1/778.1 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m866.2/866.2 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /content/lag-llama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7IUHweE4Io7",
        "outputId": "c978412b-ff62-4819-c565-36b1a67bf5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 'lag-llama.ckpt' to '/content/lag-llama/.huggingface/download/lag-llama.ckpt.b5a5c4b8a0cfe9b81bdac35ed5d88b5033cd119b5206c28e9cd67c4b45fb2c96.incomplete'\n",
            "lag-llama.ckpt: 100% 29.5M/29.5M [00:00<00:00, 47.5MB/s]\n",
            "Download complete. Moving file to /content/lag-llama/lag-llama.ckpt\n",
            "/content/lag-llama/lag-llama.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from gluonts.dataset.pandas import PandasDataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import islice\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from tqdm.autonotebook import tqdm\n",
        "from gluonts.dataset.common import ListDataset\n",
        "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
        "\n",
        "import pandas as pd\n",
        "from lag_llama.gluon.estimator import LagLlamaEstimator"
      ],
      "metadata": {
        "id": "8vw1nIbJ0vlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'dataset.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# this will give u an error if you don't put the dataset inside the lag-llama folder. Be mindful abt this"
      ],
      "metadata": {
        "id": "ZOpEla0W4MEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()\n",
        "df['ds'] = pd.to_datetime(df['ds'])\n",
        "df.set_index('ds', inplace=True)\n",
        "area_columns = [col for col in df.columns if 'AREA' in col]\n",
        "df = df[area_columns]"
      ],
      "metadata": {
        "id": "SkWo9uPU5PaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, temp_df = train_test_split(df, test_size=0.3, shuffle=False)\n",
        "validation_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle=False)\n",
        "\n",
        "train_data = ListDataset(\n",
        "    [{\"start\": train_df.index[0], \"target\": train_df[column].values} for column in train_df.columns],\n",
        "    freq=\"H\"\n",
        ")\n",
        "validation_data = ListDataset(\n",
        "    [{\"start\": validation_df.index[0], \"target\": validation_df[column].values} for column in validation_df.columns],\n",
        "    freq=\"H\"\n",
        ")\n",
        "test_data = ListDataset(\n",
        "    [{\"start\": test_df.index[0], \"target\": test_df[column].values} for column in test_df.columns],\n",
        "    freq=\"H\"\n",
        ")"
      ],
      "metadata": {
        "id": "ANyIWjwO5nn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lag_llama_predictions(dataset, prediction_length, context_length=32, num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
        "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "    estimator = LagLlamaEstimator(\n",
        "        ckpt_path=\"lag-llama.ckpt\",\n",
        "        prediction_length=prediction_length,\n",
        "        context_length=context_length,\n",
        "        input_size=estimator_args[\"input_size\"],\n",
        "        n_layer=estimator_args[\"n_layer\"],\n",
        "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "        n_head=estimator_args[\"n_head\"],\n",
        "        scaling=estimator_args[\"scaling\"],\n",
        "        time_feat=estimator_args[\"time_feat\"],\n",
        "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
        "        rope_scaling={\n",
        "            \"type\": \"linear\",\n",
        "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
        "        },\n",
        "        batch_size=batch_size,\n",
        "        num_parallel_samples=num_samples,\n",
        "    )\n",
        "\n",
        "    lightning_module = estimator.create_lightning_module()\n",
        "    transformation = estimator.create_transformation()\n",
        "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
        "\n",
        "    forecast_it, ts_it = make_evaluation_predictions(\n",
        "        dataset=dataset,\n",
        "        predictor=predictor,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
        "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
        "\n",
        "    return forecasts, tss\n"
      ],
      "metadata": {
        "id": "NZ17PzX37ROA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "prediction_length = 24\n",
        "context_length = prediction_length * 3\n",
        "num_samples = 20\n",
        "device = \"cuda\"\n",
        "\n",
        "# Fine-tune the model\n",
        "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "estimator = LagLlamaEstimator(\n",
        "    ckpt_path=\"lag-llama.ckpt\",\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    input_size=estimator_args[\"input_size\"],\n",
        "    n_layer=estimator_args[\"n_layer\"],\n",
        "    n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "    n_head=estimator_args[\"n_head\"],\n",
        "    scaling=estimator_args[\"scaling\"],\n",
        "    time_feat=estimator_args[\"time_feat\"],\n",
        "    nonnegative_pred_samples=True,\n",
        "    batch_size=64,\n",
        "    num_parallel_samples=num_samples,\n",
        "    trainer_kwargs={\"max_epochs\": 50},\n",
        ")\n",
        "\n",
        "predictor = estimator.train(train_data, cache_data=True, shuffle_buffer_length=1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "NfA7Xp6f7Vqo",
        "outputId": "35754eca-dd44-4f9d-8006-52ca7e39f4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-533b3ed399de>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lag-llama.ckpt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mestimator_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hyper_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1026\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                              \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1311\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    250\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_data,\n",
        "    predictor=predictor,\n",
        "    num_samples=num_samples\n",
        ")\n",
        "\n",
        "forecasts = list(tqdm(forecast_it, total=len(test_data), desc=\"Forecasting batches\"))\n",
        "tss = list(tqdm(ts_it, total=len(test_data), desc=\"Ground truth\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "Z1mcVW7G7YJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot(color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "W_7xetXd7m5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate metrics\n",
        "evaluator = Evaluator()\n",
        "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))\n",
        "print(agg_metrics)\n"
      ],
      "metadata": {
        "id": "ODmZ06M-7qea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "prediction_length = 24\n",
        "context_length = prediction_length * 3\n",
        "num_samples = 20\n",
        "device = \"cuda\"\n",
        "\n",
        "# Fine-tune the model\n",
        "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "estimator = LagLlamaEstimator(\n",
        "    ckpt_path=\"lag-llama.ckpt\",\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    input_size=estimator_args[\"input_size\"],\n",
        "    n_layer=estimator_args[\"n_layer\"],\n",
        "    n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "    n_head=estimator_args[\"n_head\"],\n",
        "    scaling=estimator_args[\"scaling\"],\n",
        "    time_feat=estimator_args[\"time_feat\"],\n",
        "    nonnegative_pred_samples=True,\n",
        "    batch_size=32,  # Reduced batch size for better learning\n",
        "    num_parallel_samples=num_samples,\n",
        "    trainer_kwargs={\"max_epochs\": 100},  # Increased epochs\n",
        ")\n",
        "\n",
        "predictor = estimator.train(train_data, cache_data=True, shuffle_buffer_length=1000)\n"
      ],
      "metadata": {
        "id": "HARRHf0Z8apT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_data,\n",
        "    predictor=predictor,\n",
        "    num_samples=num_samples\n",
        ")\n",
        "\n",
        "forecasts = list(tqdm(forecast_it, total=len(test_data), desc=\"Forecasting batches\"))\n",
        "tss = list(tqdm(ts_it, total=len(test_data), desc=\"Ground truth\"))\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot(color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate metrics\n",
        "evaluator = Evaluator()\n",
        "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))\n",
        "print(agg_metrics)\n"
      ],
      "metadata": {
        "id": "oegWV1fG-DJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RU1kZgAi-I3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPflvQ6QDg1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiIZLpHJDg3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'dataset.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "\n",
        "# Preprocess the data: Select only AREA columns and system column\n",
        "df = data.copy()\n",
        "df['ds'] = pd.to_datetime(df['ds'])\n",
        "df.set_index('ds', inplace=True)\n",
        "\n",
        "# Ensure 'system' column is present\n",
        "if 'System' not in df.columns:\n",
        "    raise ValueError(\"'System' column is not present in the dataset.\")\n",
        "\n",
        "# Check for missing values and fill them\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# Standardize features except the target 'System'\n",
        "scaler = StandardScaler()\n",
        "features = df.drop(columns=['System'])\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "df_scaled = pd.DataFrame(features_scaled, index=features.index, columns=features.columns)\n",
        "df_scaled['System'] = df['System']\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, temp_df = train_test_split(df_scaled, test_size=0.3, shuffle=False)\n",
        "validation_df, test_df = train_test_split(temp_df, test_size=0.5, shuffle=False)\n",
        "\n",
        "# Create ListDataset for GluonTS\n",
        "train_data = ListDataset(\n",
        "    [{\"start\": train_df.index[0], \"target\": train_df['System'].values, \"feat_static_cat\": [0]}],\n",
        "    freq=\"H\"\n",
        ")\n",
        "validation_data = ListDataset(\n",
        "    [{\"start\": validation_df.index[0], \"target\": validation_df['System'].values, \"feat_static_cat\": [0]}],\n",
        "    freq=\"H\"\n",
        ")\n",
        "test_data = ListDataset(\n",
        "    [{\"start\": test_df.index[0], \"target\": test_df['System'].values, \"feat_static_cat\": [0]}],\n",
        "    freq=\"H\"\n",
        ")"
      ],
      "metadata": {
        "id": "w5-YW6DIDg5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lag_llama_predictions(dataset, prediction_length, context_length=32, num_samples=20, device=\"cuda\", batch_size=64, nonnegative_pred_samples=True):\n",
        "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "    estimator = LagLlamaEstimator(\n",
        "        ckpt_path=\"lag-llama.ckpt\",\n",
        "        prediction_length=prediction_length,\n",
        "        context_length=context_length,\n",
        "        input_size=estimator_args[\"input_size\"],\n",
        "        n_layer=estimator_args[\"n_layer\"],\n",
        "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "        n_head=estimator_args[\"n_head\"],\n",
        "        scaling=estimator_args[\"scaling\"],\n",
        "        time_feat=estimator_args[\"time_feat\"],\n",
        "        nonnegative_pred_samples=nonnegative_pred_samples,\n",
        "        rope_scaling={\n",
        "            \"type\": \"linear\",\n",
        "            \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
        "        },\n",
        "        batch_size=batch_size,\n",
        "        num_parallel_samples=num_samples,\n",
        "    )\n",
        "\n",
        "    lightning_module = estimator.create_lightning_module()\n",
        "    transformation = estimator.create_transformation()\n",
        "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
        "\n",
        "    forecast_it, ts_it = make_evaluation_predictions(\n",
        "        dataset=dataset,\n",
        "        predictor=predictor,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    forecasts = list(tqdm(forecast_it, total=len(dataset), desc=\"Forecasting batches\"))\n",
        "    tss = list(tqdm(ts_it, total=len(dataset), desc=\"Ground truth\"))\n",
        "\n",
        "    return forecasts, tss\n"
      ],
      "metadata": {
        "id": "xl_vWdVEDlVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "prediction_length = 24\n",
        "context_length = prediction_length * 3\n",
        "num_samples = 20\n",
        "device = \"cuda\"\n",
        "\n",
        "# Fine-tune the model\n",
        "ckpt = torch.load(\"lag-llama.ckpt\", map_location=device)\n",
        "estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
        "\n",
        "estimator = LagLlamaEstimator(\n",
        "    ckpt_path=\"lag-llama.ckpt\",\n",
        "    prediction_length=prediction_length,\n",
        "    context_length=context_length,\n",
        "    input_size=estimator_args[\"input_size\"],\n",
        "    n_layer=estimator_args[\"n_layer\"],\n",
        "    n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
        "    n_head=estimator_args[\"n_head\"],\n",
        "    scaling=estimator_args[\"scaling\"],\n",
        "    time_feat=estimator_args[\"time_feat\"],\n",
        "    nonnegative_pred_samples=True,\n",
        "    batch_size=32,  # Reduced batch size for better learning\n",
        "    num_parallel_samples=num_samples,\n",
        "    trainer_kwargs={\"max_epochs\": 100},  # Increased epochs\n",
        ")\n",
        "\n",
        "predictor = estimator.train(train_data, cache_data=True, shuffle_buffer_length=1000)\n"
      ],
      "metadata": {
        "id": "OIi1PAqeDl3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "forecast_it, ts_it = make_evaluation_predictions(\n",
        "    dataset=test_data,\n",
        "    predictor=predictor,\n",
        "    num_samples=num_samples\n",
        ")\n",
        "\n",
        "forecasts = list(tqdm(forecast_it, total=len(test_data), desc=\"Forecasting batches\"))\n",
        "tss = list(tqdm(ts_it, total=len(test_data), desc=\"Ground truth\"))\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(20, 15))\n",
        "date_formater = mdates.DateFormatter('%b, %d')\n",
        "plt.rcParams.update({'font.size': 15})\n",
        "\n",
        "for idx, (forecast, ts) in islice(enumerate(zip(forecasts, tss)), 9):\n",
        "    ax = plt.subplot(3, 3, idx+1)\n",
        "    plt.plot(ts[-4 * prediction_length:].to_timestamp(), label=\"target\", )\n",
        "    forecast.plot(color='g')\n",
        "    plt.xticks(rotation=60)\n",
        "    ax.xaxis.set_major_formatter(date_formater)\n",
        "    ax.set_title(forecast.item_id)\n",
        "\n",
        "plt.gcf().tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate metrics\n",
        "evaluator = Evaluator()\n",
        "agg_metrics, ts_metrics = evaluator(iter(tss), iter(forecasts))\n",
        "print(agg_metrics)\n"
      ],
      "metadata": {
        "id": "K50OteFVDnjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QpmQwod6Dpcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}